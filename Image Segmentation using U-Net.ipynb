{"cells":[{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T16:38:01.580067Z","iopub.status.busy":"2024-04-29T16:38:01.579173Z","iopub.status.idle":"2024-04-29T16:38:01.585909Z","shell.execute_reply":"2024-04-29T16:38:01.584900Z","shell.execute_reply.started":"2024-04-29T16:38:01.580030Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import nibabel as nib\n","import cv2\n","import os\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate\n","from tensorflow.keras.models import Model\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":110,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T17:18:16.908212Z","iopub.status.busy":"2024-04-29T17:18:16.907805Z","iopub.status.idle":"2024-04-29T17:18:16.912732Z","shell.execute_reply":"2024-04-29T17:18:16.911767Z","shell.execute_reply.started":"2024-04-29T17:18:16.908183Z"},"trusted":true},"outputs":[],"source":["# Define constants\n","image_size = 128\n","num_slices = 155\n","slices_start_at = 0\n","num_channels = 4\n","num_classes = 4"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T16:38:14.699805Z","iopub.status.busy":"2024-04-29T16:38:14.699125Z","iopub.status.idle":"2024-04-29T16:38:14.707440Z","shell.execute_reply":"2024-04-29T16:38:14.706372Z","shell.execute_reply.started":"2024-04-29T16:38:14.699774Z"},"trusted":true},"outputs":[],"source":["# Load Images\n","def load_images(data_directory, sample_ids):\n","    '''\n","    Inputs:\n","        data_directory: The directory containing the dataset.\n","        sample_ids: A list of sample IDs to load.\n","    Steps:\n","        Loop Over Sample IDs: Iterate over each sample ID in the provided list.\n","        \n","        Load Image and Segmentation: For each sample ID, load the corresponding image and segmentation data from the dataset directory.\n","        \n","        Resize Images and Segmentations: Resize each image and segmentation slice to the specified image_size. The cv2.resize function is used for resizing.\n","        \n","        One-Hot Encode Segmentation: Convert the segmentation data into one-hot encoded format using tf.keras.utils.to_categorical.\n","        \n","        Append to Lists: Append the resized image slices (X) and their corresponding one-hot encoded segmentation slices (y) to separate lists.\n","        \n","        Convert to Arrays: Convert the lists X and y into numpy arrays.\n","        \n","        Normalization: Normalize the image data by dividing by the maximum pixel value, so that all pixel values will be on the same scale.\n","    Output:\n","        X: Numpy array containing the preprocessed image data.\n","        y: Numpy array containing the preprocessed segmentation data in one-hot encoded format.\n","    '''\n","    X = []\n","    y = []\n","    for sample_id in sample_ids:\n","        imgtr_path = os.path.join(data_directory, 'imagesTr', sample_id)\n","        lbltr_path = os.path.join(data_directory, 'labelsTr', sample_id)\n","\n","        imgtr = nib.load(imgtr_path).get_fdata()\n","        lbltr = nib.load(lbltr_path).get_fdata()\n","\n","        for j in range(num_slices):\n","            \n","            # Resize the j-th slice of the MRI image and append it to X\n","            X.append(cv2.resize(imgtr[:, :, j + slices_start_at], (image_size, image_size)))\n","            \n","            # Resize the j-th slice of the segmentation mask image and append its one-hot encoded version to y\n","            seg_slice = cv2.resize(lbltr[:, :, j + slices_start_at], (image_size, image_size))\n","            y.append(tf.keras.utils.to_categorical(seg_slice, num_classes))\n","\n","    X = np.array(X) / np.max(X)\n","    y = np.array(y)\n","    return X, y"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T16:38:03.519528Z","iopub.status.busy":"2024-04-29T16:38:03.519155Z","iopub.status.idle":"2024-04-29T16:38:03.531696Z","shell.execute_reply":"2024-04-29T16:38:03.530683Z","shell.execute_reply.started":"2024-04-29T16:38:03.519498Z"},"trusted":true},"outputs":[],"source":["# Model Definition\n","def create_model(input_shape):\n","    '''\n","    Input Layer: The input shape is specified by input_shape.\n","    Encoder Path:\n","        Two convolutional layers (conv1) with 32 filters each are applied with ReLU activation, followed by max-pooling (pool1) to reduce spatial dimensions.\n","        Similar operations are performed in the next set of convolutional layers (conv2 and pool2), but with 64 filters.\n","        Another set of convolutional layers (conv3) with 128 filters further reduces the spatial dimensions.\n","    Decoder Path:\n","        Upsampling (UpSampling2D) is applied to the output of conv3, followed by a convolutional layer (up1) with 64 filters.\n","        The upsampled feature map is concatenated with the feature map from conv2 (merge1), followed by convolutional layers (conv4) with 64 filters.\n","        A similar operation is performed for the next upsampled feature map (up2) and the feature map from conv1 (merge2).\n","        Convolutional layers (conv5) with 32 filters are applied to refine the segmentation.\n","    Output Layer:\n","        The final layer (output) consists of a convolutional layer with num_classes filters (in this case, 4 for multi-class segmentation) and softmax activation to produce the segmentation map.\n","    Model Compilation:\n","        The model is instantiated with the defined inputs and outputs.\n","        The model is not compiled here. Compilation typically involves specifying an optimizer, loss function, and metrics, but this is deferred to the training phase.\n","    '''\n","    inputs = Input(input_shape)\n","    conv1 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n","    conv1 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n","\n","    conv2 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n","    conv2 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n","\n","    conv3 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n","    conv3 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n","\n","    up1 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=2)(conv3))\n","    merge1 = concatenate([conv2, up1], axis=3)\n","\n","    conv4 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge1)\n","    conv4 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n","\n","    up2 = Conv2D(32, 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=2)(conv4))\n","\n","    merge2 = concatenate([conv1, up2], axis=3)\n","    conv5 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge2)\n","    conv5 = Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n","\n","    output = Conv2D(num_classes, 1, activation='softmax')(conv5)\n","    \n","    model = Model(inputs=inputs, outputs=output)\n","    return model"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T16:38:10.233793Z","iopub.status.busy":"2024-04-29T16:38:10.233416Z","iopub.status.idle":"2024-04-29T16:38:10.241079Z","shell.execute_reply":"2024-04-29T16:38:10.240036Z","shell.execute_reply.started":"2024-04-29T16:38:10.233761Z"},"trusted":true},"outputs":[],"source":["def train_model(data_directory, model, epochs, learning_rate=0.0001):\n","    '''\n","    Inputs:\n","        data_directory: The directory containing the training data.\n","        model: The model to be trained.\n","        epochs: The number of epochs for training.\n","        learning_rate: The learning rate for the Adam optimizer, with a default value of 0.0001.\n","    Steps followed in this function:\n","        Load Data: The function loads the training data using the load_data function, which loads the data from the specified directory.\n","        Preprocess Data: The training data is preprocessed using the load_images function, which prepares the input images (X_train) and their corresponding labels (y_train).\n","        Compile Model: The model is compiled using the Adam optimizer with the specified learning rate, categorical cross-entropy loss function, and accuracy metric.\n","        Model Training: The model is trained using the training data (X_train and y_train) for the specified number of epochs.\n","    '''\n","    train_samples, _ = load_data(data_directory)\n","    X_train, y_train = load_images(data_directory, train_samples)\n","\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","    model.fit(X_train, y_train, epochs=epochs)"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T16:38:15.224684Z","iopub.status.busy":"2024-04-29T16:38:15.224024Z","iopub.status.idle":"2024-04-29T16:38:15.231332Z","shell.execute_reply":"2024-04-29T16:38:15.230172Z","shell.execute_reply.started":"2024-04-29T16:38:15.224651Z"},"trusted":true},"outputs":[],"source":["def predict_image(image_path, model):\n","    '''\n","    Inputs:\n","        image_path: The path to the input image.\n","        model: The trained model used for making predictions.\n","    Steps:\n","        Load Image: Load the input image data using nibabel.\n","        \n","        Resize Image: Iterate over the slices of the input image, resize each slice to the specified image_size, and append them to a list X.\n","        \n","        Normalization: Normalize the resized image data by dividing by the maximum pixel value.\n","        \n","        Add Channel Dimension: Add a channel dimension to the preprocessed image data to match the input shape expected by the model.\n","        \n","        Make Predictions: Use the trained model to predict segmentation masks for the preprocessed image data.\n","    Outputs:\n","        X: Numpy array containing the preprocessed image data.\n","        predictions: Predicted segmentation masks for the input image data.\n","    '''\n","    img_data = nib.load(image_path).get_fdata()\n","\n","    X = []\n","    for j in range(img_data.shape[2]):\n","        X.append(cv2.resize(img_data[:, :, j], (image_size, image_size)))\n","    X = np.array(X) / np.max(X)\n","    X = np.expand_dims(X, axis=-1)  # Add channel dimension\n","\n","    predictions = model.predict(X)\n","    return X, predictions\n"]},{"cell_type":"code","execution_count":106,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T16:50:33.773108Z","iopub.status.busy":"2024-04-29T16:50:33.772735Z","iopub.status.idle":"2024-04-29T16:50:33.779647Z","shell.execute_reply":"2024-04-29T16:50:33.778758Z","shell.execute_reply.started":"2024-04-29T16:50:33.773079Z"},"trusted":true},"outputs":[],"source":["def visualize(input_image, output_image):\n","    '''\n","    Inputs:\n","        input_image: The input image data.\n","        output_image: The predicted segmentation mask.\n","    Steps:\n","        Plot Input Image: Plot the input image in the left subplot.\n","        The input image is accessed using indexing [65, :, :, 0, 0], assuming it's a 3D image with dimensions [depth, height, width, channels, samples]. Adjust the indexing according to the structure of your input data.\n","        The colormap 'gray' is used for grayscale images.\n","        Plot Output Image: Plot the output image (prediction) in the right subplot.\n","        The output image is accessed using indexing [65, :, :, 0], assuming it's a 3D segmentation mask with dimensions [depth, height, width, channels]. Adjust the indexing according to the structure of your output data.\n","        The colormap 'gray' is used for grayscale images.\n","    Display: Show the plotted images using plt.show().\n","    \n","    '''\n","    plt.figure(figsize=(10, 5))\n","\n","    # Plot the input image\n","    plt.subplot(1, 2, 1)\n","    plt.imshow(input_image[65,:,:,0,0], cmap='gray')\n","    plt.title('Input Image')\n","    plt.axis('off')\n","\n","    # Plot the output image\n","    plt.subplot(1, 2, 2)\n","    plt.imshow(output_image[65,:,:,0], cmap='gray')\n","    plt.title('Output Image')\n","    plt.axis('off')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2024-04-29T16:44:30.509125Z","iopub.status.busy":"2024-04-29T16:44:30.508715Z","iopub.status.idle":"2024-04-29T16:45:38.226817Z","shell.execute_reply":"2024-04-29T16:45:38.225949Z","shell.execute_reply.started":"2024-04-29T16:44:30.509093Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 86ms/step - accuracy: 0.9352 - loss: 1.0510\n","Epoch 2/10\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - accuracy: 0.9836 - loss: 0.0849\n","Epoch 3/10\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.9845 - loss: 0.0593\n","Epoch 4/10\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.9881 - loss: 0.0533\n","Epoch 5/10\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - accuracy: 0.9889 - loss: 0.0375\n","Epoch 6/10\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 57ms/step - accuracy: 0.9803 - loss: 0.0723\n","Epoch 7/10\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.9890 - loss: 0.0389\n","Epoch 8/10\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 57ms/step - accuracy: 0.9886 - loss: 0.0447\n","Epoch 9/10\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.9548 - loss: 0.3307\n","Epoch 10/10\n","\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 0.9900 - loss: 0.0373\n"]}],"source":["# Set the data directory\n","data_directory = \"./Task01_BrainTumour\"\n","\n","# Define input shape\n","input_shape = (image_size, image_size, num_channels)\n","\n","# Create the model\n","model = create_model(input_shape)\n","\n","# Train the model\n","train_model(data_directory, model, epochs=10)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4901941,"sourceId":8259406,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
